# RoadTalker ğŸ›£ï¸ğŸ¤–

A research project on **Vision-Language Models (VLMs)** for traffic scene understanding.
Built as part of my ML course final project and as a foundation for thesis research.

## Goals
- Explore image â†’ text alignment with a **ViT + LLM bridge** (LLaVA-style MLP, later Q-Former).
- Train on **traffic images** (BDD100K, UA-DETRAC) with auto-generated Q&A.
- Demonstrate tasks:
  - Vehicle counting
  - Type classification (car, bus, truck, etc.)
  - Color recognition
  - Basic localization (boxes)

## Structure
road_talker/
â”œâ”€â”€ data/ # datasets (ignored by git)
â”œâ”€â”€ notebooks/ # exploration + demo notebooks
â”œâ”€â”€ models/ # model components (ViT, projector, LLM)
â”œâ”€â”€ train/ # training pipeline
â”œâ”€â”€ eval/ # evaluation scripts
â”œâ”€â”€ scripts/ # helper scripts
â”œâ”€â”€ configs/ # YAML configs
â”œâ”€â”€ utils/ # helper utils
â”œâ”€â”€ tests/ # unit tests

## Setup
```bash
git clone https://github.com/lordaliH8/road_talker.git
cd road_talker

# create environment
conda env create -f environment.yml
conda activate roadstalker

# or using pip
pip install -r requirements.txt
Quickstart
Download dataset:
bash scripts/prepare_bdd100k.sh
Train with QLoRA:
bash scripts/train_qlora.sh
Run evaluation:
bash scripts/eval.sh